#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{ragged2e}
\usepackage{blindtext}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[RO,LE]{Doubly dispersive channels equalization using deep learning techniques}
\fancyfoot[CO,RE]{Emilio Tonix}
\fancyfoot{}
\fancyfoot[LE,RO]{\thepage}
\fancyfoot[CO,RE]{Emilio Tonix}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "Courier"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2.5cm
\rightmargin 2.5cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\align center
\begin_inset Graphics
	filename Imagenes/Cinvestav_Logo-transformed.jpeg
	lyxscale 13
	scale 12

\end_inset


\end_layout

\begin_layout Standard
\align center

\size larger
Centro de Investigación y de Estudios Avanzados del I.P.N 
\end_layout

\begin_layout Standard
\align center

\size larger
Unidad Guadalajara 
\end_layout

\begin_layout Standard
\align center

\end_layout

\begin_layout Standard
\align center

\series bold
\size huge
\color black
Exploring Deep Learning Techniques for Equalizing Doubly Dispersive Channels
\end_layout

\begin_layout Standard

\series bold
\color white
.
\end_layout

\begin_layout Standard
\align center

\size larger
\color black
A thesis presented by:
\end_layout

\begin_layout Standard
\align center

\series bold
\size larger
\color black
Luis Emilio Tonix Gleason
\end_layout

\begin_layout Standard
\align center

\size larger
\color black
to obtain the degree of:
\end_layout

\begin_layout Standard
\align center

\series bold
\size larger
\color black
Master in Science
\end_layout

\begin_layout Standard
\align center

\size larger
\color black
in the subject of:
\end_layout

\begin_layout Standard
\align center

\series bold
\size larger
\color black
Electrical Engineering
\end_layout

\begin_layout Standard
\align center

\size larger
\color black
Thesis Advisors:
\end_layout

\begin_layout Standard
\align center

\series bold
\size larger
\color black
Dr.
 Ramón Parra Michel
\end_layout

\begin_layout Standard
\align center

\series bold
\size larger
\color black
Dr.
 Fernando Peña Campos
\end_layout

\begin_layout Standard
\align center

\end_layout

\begin_layout Standard
\align center

\series bold
\size larger
\color black
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill Guadalajara, Jalisco 
\backslash
today
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Section*
\noindent

\size huge
\color black
Acknowledgment 
\size large
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent
\align block

\size large
Thank you for reading this; I appreciate it.
 I'm expecting that your work will benefit greatly from this material.
 I won't hesitate to say that you might have done your job more effectively
 than I did.
 Progress in science is a way to skepticism and having the best information
 from multiple sources to meet your individual standards.
  
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Section*

\size huge
Resumen 
\size large

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent
\align block

\size large
En la actualidad, la tecnología de redes de comunicación inalámbricas de
 alta movilidad está en constante evolución, y tiene aplicaciones en diversos
 ámbitos, tales como la seguridad vial, la conducción autónoma, el monitoreo
 remoto de vehículos, el vuelo de drones y los requisitos de 6G.
 Los transmisores RF, que se emplean en los sistemas de telecomunicaciones
 para transmitir información, codifican dicha información a través de la
 amplitud y la fase de una señal portadora, comúnmente conocida como datos
 IQ.
 Sin embargo, al viajar a través de un canal de alta movilidad, estos datos
 sufren distorsión, debido a la dispersión doble de dicho canal, lo que
 implica que las propiedades de la señal se ven afectadas por el desplazamiento,
 difusión en el tiempo y la frecuencia.
 Aunque existen algoritmos tradicionales que requieren ecualizadores iterativos
 complejos, que pueden ser lentos en términos de tiempo de ejecución pero
 precisos, también existen ecualizadores más sencillos, aunque menos precisos.
 En este trabajo se analizarán algunas soluciones basadas en redes neuronales,
 buscando un equilibrio entre la complejidad del tiempo y la precisión para
 hacer frente a este desafío.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\align block

\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
Es posible afirmar que los algoritmos convencionales han demostrado ser
 eficaces en el desarrollo actual.
 Sin embargo, los recientes avances en redes neuronales han simplificado
 problemas que antes eran intratables, como el descifrado de secuencias
 de ADN o la creación de imágenes a partir de texto.
 En resumen, abordan con éxito problemas no lineales.
 El objetivo de este trabajo es imitar técnicas de ecualización conocidas
 con naturaleza lineal, como los ecualizadores MSE, LMMSE y no linealies
 como MAP.
 Durante las pruebas realizaremos un seguimiento de diferentes tamaños de
 constelaciones y tasas de error de bit en una variedad de situaciones de
 interferencia y ruido.
 Esperamos que la tasa de error de bits disminuya hasta, o funcione ligeramente
 mejor que, el modelo de oro, que son los ecualizadores mencionados previamente.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\align block

\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
Los últimos enfoques en investigación literaria demuestran la existencia
 de diversas estrategias que se aproximan a lograr una buena ecualización,
 aunque no registran todas las etapas de la ecualización basándose en métodos
 tradicionales.
 Por otro lado, se presentará una estrategia que incluso aborde condiciones
 más complicadas del canal, tales como canales de línea de vista o la falta
 de ella.
 Finalmente, pero no menos importante, se busca promover una mayor investigación
 en este campo prácticamente inexplorado por algunos equipos de comunicación.
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Section*

\size huge
Abstract
\size large
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\size large
Due to their primary uses in control, road safety, autonomous driving, remote
 monitoring of vehicles, drone flying, and 6G needs, the development of
 high-mobility wireless communication networks is cutting-edge technology.
 The RF broadcasts, which are used by telecommunications systems to transfer
 encoded information over the amplitude and phase of a carrier signal, are
 normally called IQ data.
 However, this data is distorted as it travels through a high mobility channel.
 High mobility channels are doubly dispersive, which means that the signal
 properties are mixed with some temporal frequency shifting and spreading.
 There are some traditional algorithms that may require complex iterative
 equalizers that can be slow in terms of execution time.
 However, there are also simpler equalizers that may not be as precise.
 In this work, we will investigate the use of neural network-based solutions
 and consider how we should balance time complexity and accuracy to meet
 this challenge.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
We can categorically assert that conventional algorithms continue to work
 well in the context of contemporary development.
 However, recent advances in neural networks have simplified problems that
 were previously intractable, such as deciphering DNA sequences or creating
 images from text.
 In short, they successfully tackle non-linear issues.
 The objective of this work is to mimic well-known equalization techniques
 with linear properties such as MSE and LMMSE, as well as non-linear techniques
 like MAP equalizers.
 We will evaluate how well the equalization performs under various noise
 conditions and with various bit-size encodings.
 We anticipate that the bit error rate will drop to, or perform slightly
 better than, the golden model, which are the aforementioned equalizers.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
According to literature studies, there are a few cutting-edge methodologies
 that approach this goal but fall short of fully documenting all of the
 steps of equalization.
 This article presents a method for dealing with more complex circumstances,
 such as line of sight or a lack of it.
 Lastly, we aim to encourage further research into this largely untapped
 area by some communication teams.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent
\align block

\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
This chapter's main goal is to provide an overview of the technical facts
 and some of the current issues that prompted this study.
\end_layout

\begin_layout Subsection
Document organization
\end_layout

\begin_layout Standard

\size large
The structure of this document is as follows: 
\end_layout

\begin_layout Itemize

\series bold
\size large
Chapter 1 Theoretical Framework: 
\series default
Fundamental ideas and concepts behind the current project.
 Outlines the core concepts necessary to comprehend this study
\end_layout

\begin_layout Itemize

\series bold
\size large
Chapter 2 State of Art:
\series default
 Examines the existing literature on classical equalization methods and
 compares them to deep learning models.
 
\end_layout

\begin_layout Itemize

\series bold
\size large
Chapter 3 Present work:
\series default
 The present work includes the Python implementation of various models,
 to train and test different experiments, as well as an overview of the
 software architecture that makes this possible.
 The focus is on explainable deep learning models of Quadrature Amplitude
 Modulation (QAM) equalization based on neural networks, including the design
 of the proposed models.
 Additionally, the description, characteristics, and comparison with related
 work will also be discussed
\end_layout

\begin_layout Itemize

\series bold
\size large
Chapter 4 Results and Analysis:
\series default
 Explains the conception and execution of the idea, the experiments carried
 out, and the analysis of the outcomes.
 Here, we compare the BER (Bit Error Rate) graphs of our studied models.
\end_layout

\begin_layout Itemize

\series bold
\size large
Chapter 5 Conclusion and Outlook: 
\series default
The conclusions and potential future directions of this study are determined.
\end_layout

\begin_layout Subsection
\paragraph_spacing onehalf
Background
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
It takes a lot of expertise in the field of communications to model different
 types of channels, account for various channel flaws, and create the best
 signaling and detecting systems that guarantee a reliable data flow.
 The design of transmit signals in communications enables simple analytical
 techniques for symbol detection for a range of channel and system models,
 including multipath, doppler spread, and white Gaussian noise (AWGN) over
 constellation symbol detection.
 
\begin_inset CommandInset citation
LatexCommand cite
key "aghvami2005channel"
literal "false"

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
One of the guiding principles of communication system design is the division
 of signal processing into a series of distinct blocks, each of which performs
 a clearly defined and isolated functions such as source or channel coding,
 modulation, channel estimation, and equalization.
 This strategy has made it possible for us to have the effective, adaptable,
 and controlled systems we have today, but it is not certain whether individuall
y tuned processing blocks will yield the best end-to-end performance 
\begin_inset CommandInset citation
LatexCommand cite
key "proakis2002communication"
literal "false"

\end_inset

.
 During implementation, we will also consider the abstraction of the other
 communication blocks and assume that the channel has already been estimated.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Diagrams/Typical_Block_Diagram.png
	lyxscale 50
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Typical communication system block diagram 
\begin_inset CommandInset citation
LatexCommand cite
key "ChannelDiagram"
literal "false"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
On the other hand, the design philosophy guiding the creation of the 6G
 architecture will be "AI native.", allowing the network to become intelligent,
 capable of solving individual stages of the channel, and able to self-learn
 and self-adapt in response to changing network dynamics 
\begin_inset CommandInset citation
LatexCommand cite
key "The_Roadmap_to_6G"
literal "false"

\end_inset

.
 It has been demonstrated that NNs are capable of approximating any function
 
\begin_inset CommandInset citation
LatexCommand cite
key "HORNIK1989359"
literal "false"

\end_inset

, and current research has demonstrated an astounding aptitude for algorithmic
 learning 
\begin_inset CommandInset citation
LatexCommand cite
key "8697857"
literal "false"

\end_inset

.
 Due to the challenge of defining real-world images or language with strict
 mathematical models, deep learning (DL) excels in fields like computer
 vision and natural language processing.
 For example, while it is now simple to develop DL algorithms that learn
 to complete this task with accuracy greater than that of humans 
\begin_inset CommandInset citation
LatexCommand cite
key "Surpassing_Human_Level_Performance"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Despite this, we assume that the DL applications we evaluate in this thesis
 are a useful and insightful way to fundamentally rethink the problem of
 communications network design, and they show promise for performance improvemen
ts in complex communications scenarios that are difficult to describe with
 tractable mathematical models.
 Finally, we will look for the equalization stage, which, if it is seen
 in the diagram above, fits on the channel decoder.
 
\end_layout

\begin_layout Subsection
OFDM
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
Orthogonal Frequency Division Multiplexing (OFDM) is a digital communication
 technique that divides the available bandwidth into a large number of narrowban
d subcarriers, and transmits data by modulating the subcarriers with symbols.
 OFDM is widely used in wireless and wired communication systems, such as
 Wi-Fi, LTE, and DOCSIS.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent
\align block

\size large
The basic blocks of an OFDM system are:
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Diagrams/OFDM_typical.png
	lyxscale 50
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
OFDM basic diagram 
\begin_inset CommandInset citation
LatexCommand cite
key "OFDMdiagram"
literal "false"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\series bold
\size large
Binary sequence
\series default
: This block generates the data to be transmitted.
 The data is usually a sequence of bits.
\end_layout

\begin_layout Itemize
\align block

\series bold
\size large
QAM mapping
\series default
: Bits are grouped into blocks called symbols usally named as alphabet 
\begin_inset Formula $\boldsymbol{\mathbb{A}}$
\end_inset

.
 We have a set of bits 
\begin_inset Formula $2^{\mathbb{A}}$
\end_inset

 grouped inside the alphabet.
 
\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\align block

\series bold
\size large
IFFT block
\series default
: This block applies an Inverse Fast Fourier Transform (IFFT) to the modulated
 symbols, dividing them into a set of subcarriers.
\end_layout

\begin_layout Itemize
\align block

\series bold
\size large
Cyclic prefix
\series default
: Helps OFDM symbols to reduce inter-symbol interference.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
At regular intervals, the transmitter inserts recognizable symbols known
 as "pilots" into the transmitted signal.
 These pilots are selected to have a known value and to be separated from
 one another by a given number of symbols.
 By comparing the known transmission symbols to the received symbols, the
 receiver can then utilize the pilots to estimate the channel.
 This may lower the system's data throughput.
 Subsequently, the equalizer performs channel equalization using the coefficient
s obtained by the estimator, to reduce the distortions caused by the communicati
on channel in the received data.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Since we have been provided with a dataset that contains previously estimated
 channels, we will not prioritize the channel estimation component of this
 project.
 Rather, in order to minimize errors and impairments at the receiver and
 to transmit the symbols as accurately as possible, we will focus on canceling
 the effects of the received, but well-known, channel during the communication
 process.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
We are entering a new research area as we attempt to generalize the pseudo
 inverse of the channel to cancel its effects.
 It is well known that the matrix inverse is not a continuous function,
 as some matrices may not have an inverse or may be singular.
 However, we are using a subset of invertible matrices to mitigate this
 issue, and also considering realistic physical phenomena.
 Additionally, we must grapple with the numerical instability of the matrix
 inverse, as well as the use of complex numbers in a neural network.
\end_layout

\begin_layout Subsubsection
Advantages and Disadvantages in OFDM
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
The advantages offered by OFDM systems in broadband systems are as follows
 
\begin_inset CommandInset citation
LatexCommand cite
key "ofdm_book"
literal "false"

\end_inset

:
\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\align block

\series bold
\size large
High spectral efficiency
\series default
: OFDM can transmit a large amount of data over a wide frequency band by
 dividing the available bandwidth into multiple narrowband subcarriers.
\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\align block

\series bold
\size large
Robustness to channel impairments
\series default
: OFDM is less sensitive to frequency-selective fading and interference
 than other multiplexing techniques, making it well-suited for use in wireless
 communication systems.
\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\align block

\series bold
\size large
Ease of implementation
\series default
: OFDM can be implemented using simple digital signal processing techniques,
 making it relatively easy to design and implement.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
Some disadvantages of OFDM include:
\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\align block

\series bold
\size large
High peak-to-average power ratio
\series default
: OFDM signals have a high peak-to-average power ratio, which can cause
 problems in power amplifier systems and limit the range of the transmitted
 signal.
\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\align block

\series bold
\size large
Sensitivity to timing errors: 
\series default
OFDM is sensitive to timing errors, which can cause inter-symbol interference
 and reduce the performance of the system.
\end_layout

\begin_layout Itemize

\series bold
\size large
Sensitivity to Doppler spread:
\series default
 Doppler spread causes interference between the subcarriers.
\end_layout

\begin_layout Subsection
QAM and IQ data
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
QAM (Quadrature Amplitude Modulation) is a type of digital modulation that
 encodes data onto a carrier signal by modulating the amplitude and phase
 of the signal.
 It is commonly used in digital communication systems to transmit digital
 data over analog channels.
 IQ data refers to the in-phase and quadrature components of a complex-valued
 signal.
 In digital communication systems, the IQ data is typically used to represent
 the amplitude and phase of the modulated carrier signal.
 It's ussualy represented with complex numbers in an alphabet 
\begin_inset Formula $\mathbb{A}\in\mathbb{C}^{n}$
\end_inset

.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Diagrams/16QAM.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
16-QAM constellation 
\begin_inset CommandInset citation
LatexCommand cite
key "wiki_16qam"
literal "false"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\size large
 In the field of communication systems, a QAM constellation refers to a
 graphical representation of the symbol points in an alphabet on the complex
 plane.
 Each point represents a specific combination of the in-phase and quadrature
 components of the modulated signal.
 The number of points in the constellation is determined by the number of
 possible combinations of in-phase and quadrature values, which is in turn
 determined by the number of bits per symbol used in the QAM modulation
 scheme.
 For instance, in a 16-QAM constellation, there are 16 symbol points arranged
 in a square grid, with each point corresponding to 4 bits of data.
 The distance between points in the constellation serves as an indicator
 of the signal-to-noise ratio required for reliable transmission of the
 data.
 It is common for QAM constellations to utilize gray code for assigning
 the symbol points in a manner that minimizes the error rate.
 However, this is not the only method that can be employed for this purpose
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/QAM_noise.jpg
	lyxscale 80
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
QPSK and 16 QAM with noise and phase displacement 
\begin_inset CommandInset citation
LatexCommand cite
key "QAMwithNoise"
literal "false"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
SNR
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
SNR stands for Signal-to-Noise Ratio which contrasts the strength of the
 signal with the strength of the noise.
 The most common way to measure it is in decibels (dB).
 In general, higher numbers indicate a better specification because there
 is a greater ratio of useful information (the signal) to unwanted data
 (the noise).
 The following equation displays the necessary power spectral density of
 the noise.
 
\begin_inset CommandInset citation
LatexCommand cite
key "proakis2002communication"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{N_{0}=\frac{P_{signal}}{P_{noise}}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{P_{signal}=\sum|s|^{2}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
For realistic simulations, noise is typically an AWGN, which is useful.
 In order to simulate bit error rates, AWGN can generate appropriate power
 levels.
 Typically, noise power is determined by its variance 
\begin_inset Formula $\boldsymbol{\sigma^{2}}$
\end_inset

, which in simulations is used as follows.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{\sigma^{2}=10^{\frac{SNR_{dB}}{10}}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Misunderstandings have arisen regarding the signs assigned to the positive
 and negative exponents in the equation.
 While there are conflicting explanations in various sources, to clear up
 any confusion, it's important to consider the visual representation of
 the equation
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{N_{0}=\frac{P_{signal}}{P_{noise}}=\frac{\sum|s|^{2}}{10^{\frac{SNR_{dB}}{10}}}=\sum|s|^{2}\times10^{-\frac{SNR_{dB}}{10}}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The higher the SNR, the better the quality of the signal, since the noise
 is relatively weaker.
 In practice, the SNR is usually calculated for a specific frequency band
 or for a specific signal-processing method.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The necessary noise variance (noise power) for producing Gaussian random
 noise, assuming complex IQ plane for all digital modulations, is given
 by
\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{n=\sqrt{(N_{0}/2)}*(randn(size(s))+j*randn(size(s)))}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\size large
with the recived signal 
\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{r=s+n}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
\paragraph_spacing onehalf
Channel
\size large
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Dispersion and doubly dispersion
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
Dispersion, in the context of radio communication systems, refers to the
 spreading out of a signal over a range of frequencies or wavelengths as
 it travels through a medium, such as air.
 This phenomenon can lead to distortion of the signal and negatively impact
 the performance of the communication system.
 There are various factors that can contribute to dispersion, including
 the distance the signal travels, the presence of obstacles or reflections,
 and the frequency of the signal.
 Techniques such as equalization, adaptive modulation, and error correction
 codes can be used to mitigate the effects of dispersion.
 A doubly dispersive channel is a type of communication channel that exhibits
 dispersion in two dimensions, such as time and frequency.
 This means that the signals transmitted through the channel are spread
 out in both time and another aspect, such as frequency or spatial dimensions.
 
\begin_inset CommandInset citation
LatexCommand cite
key "rappaport2002wireless"
literal "false"

\end_inset


\end_layout

\begin_layout Subsubsection
Multipath fading and doppler shift
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent
\align block

\size large
A multipath fading channel is a type of wireless communication channel that
 experiences fading of the transmitted signal due to multiple paths of the
 signal between the transmitter and receiver.
 This can occur when the signal reflects off of obstacles such as buildings
 or terrain, or when it is refracted by the atmosphere.
 Multiple paths of the transmitted signal can cause constructive and destructive
 interference at the receiver, resulting in rapid fluctuations in the received
 signal strength.
 This can cause the signal to fade in and out, which can affect the quality
 and reliability of the communication.
 
\begin_inset CommandInset citation
LatexCommand cite
key "balanis_2012"
literal "false"

\end_inset

.
 We begin with the ray-tracing technique and make advantage of the physical
 geometry of the propagation environment to create a deterministic model
 of the wireless channel.
 The delay of a signal refers to the time it takes for the signal to travel
 from the transmitter to the receiver, and the Doppler shift of a signal
 refers to the frequency shift of the signal due to the relative motion
 between the transmitter and receiver.
\size default

\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Diagrams/Propagation.png
	lyxscale 50
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Delay multipath 
\begin_inset CommandInset citation
LatexCommand cite
key "hong_thaj_viterbo_2022"
literal "false"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{\mathbf{r(t)=g_{1}s(t-\tau_{1})+g_{2}s(t-\tau_{2})}}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\mathbf{r(t)}$
\end_inset

 recieved signal
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\mathbf{g_{n}}$
\end_inset

 baseband equivalent complex gain(attenuation)
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\mathbf{\boldsymbol{\tau_{1}=\frac{r_{1}}{c}}}$
\end_inset

 where c is the speed of light
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\mathbf{\boldsymbol{\tau_{2}=\frac{(r_{2}+r_{3})}{c}}}$
\end_inset

 delay in reflected path
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\boldsymbol{\mathbf{\tau_{2}-\tau_{1}}}$
\end_inset

 delay spread
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
In wireless communication, the Doppler shift can cause changes in the frequency
 of a signal as it travels from the transmitter to the receiver.
 This can happen when the transmitter or receiver (or both) are moving relative
 to each other, causing the frequency of the signal to shift.
 The Doppler shift can affect the performance of a wireless communication
 system by causing changes in the signal-to-noise ratio and the signal-to-interf
erence ratio, which can degrade the quality of the signal and make it more
 difficult to detect and decode.
\begin_inset CommandInset citation
LatexCommand cite
key "marsland2013radio"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Diagrams/DopplerBasic.png
	lyxscale 50
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Doppler shifts due to the different angles of arrival 
\begin_inset CommandInset citation
LatexCommand cite
key "hong_thaj_viterbo_2022"
literal "false"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{\mathbf{r(t)=g_{1}e^{j2\pi\nu_{1}(t-\tau_{1})}s(t-\tau_{1})+g_{2}e^{j2\pi\nu_{2}(t-\tau_{2})}s(t-\tau_{2})}}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\mathbf{\boldsymbol{\nu_{1}=\frac{v}{c}f_{c}}}$
\end_inset

 LOS doppler shift
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\mathbf{\boldsymbol{\nu_{1}=\frac{v*cos(\theta)}{c}f_{c}}}$
\end_inset

 doppler shift in reflected path
\end_layout

\begin_layout Standard
\align block

\size large
We can generalize for time dependent function the gain as follows: 
\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{\mathbf{g(\tau_{i},t)=g_{i}e^{j2\pi\nu_{i}(t-\tau_{i})}}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent
\align block

\size large
Therefore impulse time-frequency response of channel at fixed time t, can
 be obtained by taking fourier transform along the delay dimension of 
\begin_inset Formula $\mathbf{\boldsymbol{g(\tau,t)}}$
\end_inset


\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\mathbf{\boldsymbol{H(f,t)=\int g(\tau,t)e^{-j2\pi f\tau}d\tau}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent
\align block

\size large
A time-frequency channel is a type of communication channel that is characterize
d by time-varying frequency-selective fading.
 This means that the channel experiences changes in its frequency response
 over time, resulting in variations in the amplitude and phase of the signals
 transmitted through it.
 Because a channel is assumed to have a slow time-varying function of t,
 we refer to this phenomenon as having a wide sense of being stationary.
 
\begin_inset CommandInset citation
LatexCommand cite
key "rappaport2002wireless"
literal "false"

\end_inset


\end_layout

\begin_layout Subsubsection
Wide sense stationary
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
A wide sense stationary (WSS) process is a stochastic process in which the
 mean and the autocorrelation function of the process do not change over
 time.
 As a result, the process' statistical characteristics, such as the mean,
 variance, and autocorrelation, remain consistent across time.
 In signal processing and telecommunications, WSS processes are frequently
 used to simulate signals that are stationary over a period of time.
 A generalization of strictly stationary processes, or processes in which
 the mean and auto-correlation function are constant over time, are WSS
 processes.
 
\begin_inset CommandInset citation
LatexCommand cite
key "papoulis2002probability"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent
\align block

\size large
A wide sense stationary (WSS) process can be described by the following
 equations:
\end_layout

\begin_layout Enumerate
\paragraph_spacing onehalf
\noindent
\align block

\size large
The mean of the process is constant over time, and can be represented by
 the equation:
\end_layout

\begin_deeper
\begin_layout Standard

\size large
\begin_inset Formula 
\begin{equation}
\mathbf{\boldsymbol{E[X(t)]=\mu}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\size large
Where 
\begin_inset Formula $\boldsymbol{E[X(t)]}$
\end_inset

 is the expected value of the process at time t, and 
\begin_inset Formula $\boldsymbol{μ}$
\end_inset

 is the constant mean of the process.
\end_layout

\end_deeper
\begin_layout Enumerate
\paragraph_spacing onehalf
\noindent
\align block

\size large
The autocovariance of the process, which is a measure of the correlation
 between two points in time, is also constant over time, and can be represented
 by the equation:
\end_layout

\begin_deeper
\begin_layout Standard

\size large
\begin_inset Formula 
\begin{equation}
\boldsymbol{R_{X}(t1,t2)=E[(X(t_{1})-\mu)(X(t_{2})-\mu)]=R(t_{1}-t_{2})}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
Where
\begin_inset Formula $\boldsymbol{R_{X}(t_{1},t_{2})}$
\end_inset

is the autocovariance of the process at times t1 and t2, and 
\begin_inset Formula $\boldsymbol{R(t_{1}-t_{2})}$
\end_inset

 is the autocovariance function of the process, which is a function of the
 time difference between t1 and t2
\end_layout

\end_deeper
\begin_layout Enumerate
\paragraph_spacing onehalf
\noindent
\align block

\size large
The autocorrelation function of the process, which is a measure of the correlati
on between two points in time, is also constant over time, and can be represente
d by the equation
\end_layout

\begin_deeper
\begin_layout Standard

\size large
\begin_inset Formula 
\begin{equation}
\boldsymbol{r_{X}(t1,t2)=\frac{R_{X}(t_{1},t_{2})}{\sqrt{R_{X}(t_{1},t_{1})R_{X}(t_{2},t_{2})}}=r(t_{1}-t_{2})}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
Where 
\begin_inset Formula $\mathbf{r_{X}(t_{1},t_{2})}$
\end_inset

 is the autocorrelation function of the process at times t1 and t2, and
 
\begin_inset Formula $\boldsymbol{r(t_{1}-t_{2})}$
\end_inset

 is the autocorrelation function of the process, which is a function of
 the time difference between t1 and t2.
\end_layout

\end_deeper
\begin_layout Subsection
Equalization
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
A telecom channel equalizer is a device or algorithm used in telecommunications
 systems to compensate for distortion or other impairments in a communication
 channel.
 The equalizer uses signal processing techniques to estimate the characteristics
 of the channel, such as the impulse response or the frequency response,
 and then applies a correction to the transmitted signal to counteract the
 effects of the channel on the received signal.
 This can improve the performance of the communication system by reducing
 errors and increasing the data rate or signal-to-noise ratio.
 Bluetooth, WiFi, IOT, drones, V2V, wireless broadband, and satellite communicat
ions are just a few of the everyday applications they can be used for.
 
\begin_inset CommandInset citation
LatexCommand cite
key "goldsmith_2005"
literal "false"

\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Diagrams/Equalizer_Basic.jpg
	lyxscale 30
	scale 15

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Basic Equalizer
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Our goal is to develop an equalizer based on deep learning techniques and
 research how it performs in terms of timing and memory complexity.
 We take as a reference the classical methods that manage a good bit error
 rate, and we will take them as a golden model of accuracy.
 
\end_layout

\begin_layout Subsubsection
Estimator
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
An estimator is a statistical function that maps an observation to an estimate
 of a parameter of the signal being observed, normally known as 
\size larger

\begin_inset Formula $\hat{\boldsymbol{\theta}}$
\end_inset


\size large
 .It is a mathematical function that takes the data, usually in the form
 of a sample, and produces an estimate of an unknown parameter of the underlying
 probability distribution.
 It's important to note that the quality of an estimator is usually measured
 by some metric such as Mean Squared Error (MSE), Mean Absolute Error (MAE)
 or Maximum Likelihood (ML) that indicates how well the estimator is able
 to estimate the true parameter 
\begin_inset CommandInset citation
LatexCommand cite
key "MSE_MMSE"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Zero forcing
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The Zero Forcing Equalizer (ZFE) is a technique used in communication systems
 to reduce the impact of intersymbol interference (ISI), caused by the presence
 of multiple signal paths in a communication channel.
 Although the ZFE is effective, it has some limitations, such as being susceptib
le to noise and unable to handle certain types of channel distortions.
 Nevertheless, it can improve the performance of communication systems in
 specific scenarios.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
In our case, we are dealing with a matrix channel that is already in the
 frequency domain.
 To accelerate the ZFE process, we extract the main diagonal of the channel
 matrix using the "diag(H)" operation.
 Then, we apply the operation of "Y" divided by "diag(H)" to obtain the
 estimated transmitted symbols, where 
\begin_inset Formula $\boldsymbol{Y=Hx+w}$
\end_inset

.
 The H matrix, which multiplies the input data x, is almost canceled out
 in the final approach.
 Specifically, we have 
\begin_inset Formula $\hat{x}\simeq x+\frac{w}{diag(H)}$
\end_inset

, where w is the noise vector and diag(H) is the main diagonal of the channel
 matrix H.
 This expression shows that the estimated transmitted symbols are obtained
 by adding the input data to the noise vector w divided by the main diagonal
 of the channel matrix.
 In the equations bellow the estimator is shown.
 
\end_layout

\begin_layout Standard
\align center

\size large
\begin_inset Formula 
\begin{equation}
\boldsymbol{\hat{x}=\frac{Y}{diag(H)}=\frac{Hx+w}{diag(H)}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent

\size large
Based on a paper, we will use zero forcing as a preprocessing stage for
 some of our experiments
\begin_inset CommandInset citation
LatexCommand cite
key "ZeroForcingEqNN"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection
MSE
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
MSE equalization, or minimum mean square error, is a posterior estimator.It
 is a measure of how well the estimator, denoted by 
\size larger

\begin_inset Formula $\hat{\boldsymbol{\theta}}$
\end_inset


\size large
, is able to estimate the true parameter, denoted by 
\size larger

\begin_inset Formula $\boldsymbol{\theta}$
\end_inset


\size large
, based on the observed data .It is calculated after the estimator has been
 applied to the data and produces a single scalar value that indicates how
 far the estimated values are from the true values on average.
\begin_inset CommandInset citation
LatexCommand cite
key "MSE_MMSE"
literal "false"

\end_inset

 This technique is commonly used in digital communication systems to improve
 the performance of an equalizer by reducing the mean square error (MSE)
 between the transmitted and received signals.
 It can provide significant improvements in the performance of the channel
 equalization by reducing the effects of noise, interference, and other
 impairments 
\begin_inset CommandInset citation
LatexCommand cite
key "proakis2002communication"
literal "false"

\end_inset

.
 In the context of machine learning, our neural network will be a non-linear
 estimator and can be evaluated by this estimator.
 
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align center

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{MSE(\hat{\theta})=E[(\theta-\hat{\theta})^{2}]}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\align block

\size large
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 is the transmitted signal with the original values
\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\align block

\size large
\begin_inset Formula $\boldsymbol{\hat{\theta}}$
\end_inset

 is the predicted values of the received signal.
\end_layout

\begin_layout Subsubsection
MMSE
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
Since MMSE equalization is a linear equalization technique, the transmitted
 signal is first adjusted using a linear filter before being sent across
 the channel.
 By choosing the proper filter coefficients for the linear filter, the purpose
 of MMSE equalization is to reduce the MSE between the sent and received
 signals.
 The MMSE criteria, which stipulates that the filter coefficients should
 be set to minimize the MSE between the sent and received signals, may be
 used to compute the filter coefficients.
 The equation below can be used to do this: 
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align center

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{MMSE=min(E[(\theta-\hat{\theta})^{2}])}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\align block

\size large
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 is the transmitted signal.
\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\align block

\size large
\begin_inset Formula $\boldsymbol{y}$
\end_inset

 is the received signal.
\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\align block

\size large
\begin_inset Formula $\boldsymbol{\hat{\theta}=E[x|y]}$
\end_inset

 is an estimator given by Y to an estimate the X
\end_layout

\begin_layout Subsubsection
LS
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
Least squares (LS) equalization is a linear equalization method that aims
 to minimize the mean squared error (MSE) between the estimated and the
 transmitted symbols.
 The noise component of x is disregarded by the LS (Least-Squares) equalizer,
 which treats it as a deterministic variable.
 A linear filter 
\begin_inset Formula $\boldsymbol{W}$
\end_inset

 that minimize the mean square error between 
\begin_inset Formula $\boldsymbol{\hat{x}}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{Hx}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
The argmin function is a mathematical function that returns the indices
 of the minimum values of a given array or matrix.
 It is often used in optimization problems, where it is used to find the
 values of the variables that minimize some objective function.
\begin_inset CommandInset citation
LatexCommand cite
key "argmin_reference"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{W_{Ls}=argmin||\hat{x}-Hx||^{2}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
With the solution in the Moore-Penrose pseudoinverse 
\begin_inset Formula $\boldsymbol{H^{+}}$
\end_inset

.The Moore-Penrose pseudoinverse is often used to solve linear least squares
 problems, which involve finding the values of variables that minimize the
 sum of the squares of the residuals (the differences between the observed
 values and the values predicted by the model).
 It can also be used to compute a "best fit" solution for systems of linear
 equations that do not have a unique solution.
 
\begin_inset CommandInset citation
LatexCommand cite
key "strang2019linear"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{H^{+}=(H^{H}H)^{-1}H^{H}}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\boldsymbol{H}$
\end_inset

 Channel matrix
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\boldsymbol{H^{+}}$
\end_inset

Moore-Penrose pseudoinverse 
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\boldsymbol{H^{H}}$
\end_inset

 Hermiatian transpose matrix.
 Complex square matrix that is equal to its own conjugate transpose
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
Its resistance to noise makes it appealing in a variety of communication
 systems, however because the noise component is ignored, it cannot operate
 satisfactorily with low SNR (signal to noise ratio).
 
\end_layout

\begin_layout Subsubsection
LMMSE
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
A form of linear filter called the Linear Minimum Mean Squared Error (LLMSE)
 Equalizer seeks to reduce the mean squared error (MSE) between the actual
 signal 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 and an estimation of the signal 
\begin_inset Formula $\boldsymbol{\hat{x}}$
\end_inset

.
 Incorporating second-order statistics of the data and the noise and treating
 the noise as a random variable allows it to achieve this.
 Compared to the Least Squares (LS) algorithm, this can perform better when
 there is low signal-to-noise ratio (SNR).
 To put it another way, the LLMSE equalization is a method that can be applied
 to restore precision to a signal that has been distorted by noise, especially
 when the noise level is high.
 When there is a low signal-to-noise ratio (SNR) and a significant quantity
 of noise in the signal, it performs exceptionally well.
\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{W_{LMMSE}=argminE||x-\hat{x}||^{2}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
This equation consider AWGN (Additive White Gaussian Noise) with variance
 
\begin_inset Formula $\boldsymbol{\sigma^{2}}$
\end_inset

.
 It is called "white" because it has a flat power spectral density, meaning
 that it has equal power at all frequencies.
 It is called "additive" because it can be added to a signal without changing
 its distribution.
 
\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{W_{LMMSE}=(H^{H}H+\sigma^{2}I)^{-1}H^{H}}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\boldsymbol{\sigma^{2}}$
\end_inset

 variance of AWGN
\end_layout

\begin_layout Subsubsection
Pros and Cons
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
As we can see in the previous methods, the calculation of the equalization
 matrices is mainly based on finding the pseudo-inverse of certain matrix
 factors.
 However, directly solving for the inverse of the matrix can be computationally
 complex 
\begin_inset Formula $O(N^{3})$
\end_inset

 and, in addition, numerically unstable.
 This occurs if the matrix has a very small determinant, in which case the
 true solution may be subject to large perturbations.
 This will lead to a very complicated circuit architecture for numerical
 calculation.
\end_layout

\begin_layout Subsection

\size large
Neuronal
\size default
 networks
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Neural networks are a type of artificial intelligence system designed to
 mimic the functioning of the human brain.
 They consist of interconnected nodes, or "neurons," which are capable of
 processing information and making decisions based on that information.
 These networks are usually organized into layers, with each layer containing
 a different number of neurons.
 The input layer receives input from the external environment, while the
 output layer produces the final result or decision based on that input.
 The layers in between the input and output layers are called hidden layers,
 and they perform various intermediate calculations and processing tasks.
 Neural networks are trained using large amounts of data, allowing them
 to learn and make predictions or decisions based on that data.
 In this study case, the data consists of realistic channel realizations.
\end_layout

\begin_layout Subsubsection
Linear Layer
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
A linear layer in a neural network is a type of layer that applies a linear
 transformation to the input data.
 This transformation can be represented by a 
\series bold
matrix
\series default
 of weights, denoted as 
\size larger

\begin_inset Formula $\boldsymbol{W_{n}}$
\end_inset


\size large
 ,and a biases 
\series bold
vector
\series default
, denoted as
\size larger
 
\begin_inset Formula $\boldsymbol{b_{n}}$
\end_inset


\size large
, which are learned during the training process.
 The subscript 
\size larger

\begin_inset Formula $\boldsymbol{n}$
\end_inset


\size large
 refers to the nth layer.
 The output of a linear layer is calculated by performing a matrix and vector
 product between the input data 
\size larger

\begin_inset Formula $\boldsymbol{X_{n-1}}$
\end_inset

 
\size large
and the weights 
\size larger

\begin_inset Formula $\boldsymbol{W_{n}}$
\end_inset


\size large
 as well as adding the biases.
 
\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{X_{n}=W_{n}X_{n-1}+b_{n}}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize

\size larger
\begin_inset Formula $\boldsymbol{W_{n}}$
\end_inset


\size large
weight matrix at layer n.
\end_layout

\begin_layout Itemize

\size larger
\begin_inset Formula $\boldsymbol{b_{n}}$
\end_inset

 
\size large
bias at layer n
\end_layout

\begin_layout Itemize

\size larger
\begin_inset Formula $\boldsymbol{X_{n-1}}$
\end_inset


\size large
Input or last layer data
\end_layout

\begin_layout Itemize

\size larger
\begin_inset Formula $\boldsymbol{X_{n}}$
\end_inset

 
\size large
Output data
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Diagrams/NN_eq.png
	lyxscale 70
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Output as Y and input as X.
 Vector matrix representation of system.
 Desing done with manim
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Backpropagation 
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
The goal of the layer is to optimize the weights parameters so that they
 fit a target referred to as the ground truth.
 The error, denoted by E, is calculated as the difference between the predicted
 output (
\begin_inset Formula $\hat{X}$
\end_inset

) and the actual target output (
\begin_inset Formula $X_{n}$
\end_inset

).
 To achieve this, we will utilize the backpropagation algorithm, which is
 a common method in the field of artificial neural networks for training
 the network by adjusting the weights between neurons in the network.
 
\size larger

\begin_inset Formula 
\begin{equation}
\boldsymbol{Err=\hat{X}-X_{n}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
This method analyzes the error rate in relation to the weights and inputs.
 As we adjust our trainable parameters, {
\size larger

\begin_inset Formula $\boldsymbol{W_{n}}$
\end_inset


\size large
, 
\size larger

\begin_inset Formula $\boldsymbol{b_{n}}$
\end_inset


\size large
}, the error will change accordingly.
 The goal is to minimize the error, or to find a point where the error gradient
 is zero.
\begin_inset CommandInset citation
LatexCommand cite
key "Goodfellow-et-al-2016"
literal "false"

\end_inset


\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{\nabla W=\frac{\partial E}{X_{n\text{+1}}}\times X_{n-1}^{T}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{\nabla X_{n-1}=W_{n}^{T}\times\frac{\partial E}{X_{n\text{+1}}}}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize

\size larger
\begin_inset Formula $\boldsymbol{\nabla W}$
\end_inset


\size large
Gradient of weights.The gradient is a multi-variable generalization of the
 derivative.
\end_layout

\begin_layout Subsubsection
Learning rate
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
During the process of updating the weights, the learning rate is a hyperparamete
r that determines the size of the update step applied during the training
 of a neural network..
 It is a scalar value that is used to multiply the gradient of the loss
 function with respect to the weights of the network during gradient descent.
 The learning rate determines how fast the weights of the network are updated
 during training.
 A smaller learning rate results in slower updates, while a larger learning
 rate results in faster updates.
 The learning rate is typically set manually, and finding the optimal learning
 rate for a given problem is an important aspect of training a neural network.

\size default
 
\begin_inset CommandInset citation
LatexCommand cite
key "learningrate"
literal "false"

\end_inset

 
\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{W_{n}=W_{n}-\gamma\nabla W}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{b_{n}=b_{n}-\gamma\frac{\partial E}{X_{n}}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
If the learning rate is set too low, the optimization process may become
 stuck in a local minimum or local maximum.
 A local minimum is a point in the optimization landscape where the cost
 function has a lower value than the surrounding points, but is not the
 global minimum.
 A local maximum is a point where the cost function has a higher value than
 the surrounding points, but is not the global maximum.
 This can lead to suboptimal performance or even failure of the optimization
 process.
 On the other hand, if the learning rate is set too high, the optimization
 process may oscillate or diverge, also leading to suboptimal performance.
 It is important to choose an appropriate learning rate for the optimization
 process in order to avoid these problems.
\end_layout

\begin_layout Subsubsection
Basic code implementation
\end_layout

\begin_layout Standard

\size large
Although it may seem difficult to implement the code, it is actually quite
 simple as it involves encapsulating the results of the previous layer's
 calculations.
\end_layout

\begin_layout Standard

\size large
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
lstinputlisting[language=python]{Codigo/Linear.py}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
And a basic implmentation of the Linear Class.
\end_layout

\begin_layout Standard

\size large
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
lstinputlisting[language=python]{Codigo/TestLinear.py}
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Activation functions 
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Activation functions are used in neural networks to introduce non-linearity
 into the network.
 This is important because many real-world problems are non-linear in nature
 and a neural network with only linear functions would not be able to model
 such problems accurately.
 Activation functions allow the network to learn more complex patterns in
 the data and improve the accuracy of the network.
 They also help to prevent the network from becoming stuck in a local minimum
 or plateau during training.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/ActivationFunctions.png
	lyxscale 10
	scale 10

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Activation function.
 (a) Sigmoid, (b) tanh, (c) ReLU.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ActivationFunctionsImages"
literal "false"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The most popular activation functions in neural networks are the sigmoid
 function, the hyperbolic tangent (tanh) function, and the rectified linear
 unit (ReLU) function.
 The sigmoid function maps any input value to a range between 0 and 1, while
 the tanh function maps input values to the range between -1 and 1.
 The ReLU function is a linear function that maps all negative input values
 to 0 and all positive input values to their original value.
\begin_inset CommandInset citation
LatexCommand cite
key "ActivationFunctions"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
However, there are "hardened" versions of activation functions that can
 be evaluated computationally faster yet produce similar results.
 Examples of these include hardtanh and hardsigmoid.
 The Hardtanh function is defined as: 
\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{hardtanh(x)=\begin{cases}
-1, & \text{if }x<-1\\
x, & \text{if }-1\le x\le1\\
1, & \text{if }x>1
\end{cases}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Hardtanh.png
	lyxscale 25
	scale 25

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Hardtanh(red) and Tanh(blue)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Loss functions
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
A loss function is often defined as a scalar function of the model's parameters,
 the input data, and the true output.
 It quantifies how well the model is able to fit the data, and it's commonly
 used to evaluate the performance of different models and to select the
 best one.
 In terms of estimators, a loss function can be seen as a measure of how
 well the estimator is able to estimate the true parameter.
 The goal of training a machine learning model is to find the optimal set
 of parameters that minimize the loss function.
 There are different types of loss functions, each one is suitable for different
 types of problems.
 For example, in a regression problem, the mean squared error (MSE) is a
 commonly used loss function, while in a classification problem, the cross-entro
py loss is often used.
 
\end_layout

\begin_layout Subsubsection
Cross Entropy Loss
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Previously, we have discussed the Mean Squared Error (MSE) in the context
 of estimators.
 However, there is another cost function, known as the cross-entropy loss,
 which can be utilized for evaluating models in the context of classification
 problems.
 Prior to elaborating on the cross-entropy loss, it is imperative to also
 discuss the softmax function.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The softmax function is a mathematical technique which transforms a vector
 of real numbers into a probability distribution over the classes.
 The output of the softmax function is a vector of values between 0 and
 1 that sum up to 1, which can be interpreted as probabilities.
 The softmax function is employed in this context as it provides a means
 to convert the output, or scores, of the model into a probability distribution
 that represents the uncertainty of the model's predictions.
 Additionally, the softmax function ensures that the probability of each
 class falls within the range of 0 and 1 and that the sum of all class probabili
ties is 1, which is a necessary requirement for a probability distribution.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/softmax.png
	lyxscale 40
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Softmax used to map real numbers to probabilty distribution.
 
\begin_inset CommandInset citation
LatexCommand cite
key "softmax"
literal "false"

\end_inset

 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
After the softmax output, it is measured by the cross-entropy loss function,
 which checks the dissimilarity between the predicted and true probability
 distributions.
 One of the many advantages of this function is that it is also easy to
 compute and differentiate, making it a suitable loss function for gradient-base
d optimization algorithms.
 The goal of the training process is to minimize the Kullback-Leibler divergence.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{D_{KL}(P||Q)=\sum P(i)log\frac{P(i)}{Q(i)}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
This equation is also known as relative entropy.
 This function will determine which of the given classes is more likely.
 The predicted class distribution is represented by 
\begin_inset Formula $P(y|x_{i};\theta)$
\end_inset

 where 
\begin_inset Formula $\theta$
\end_inset

 are the parameters and the true class distribution is represented by 
\begin_inset Formula $P^{*}(y|x_{i})$
\end_inset

 which are taken into consideration.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{D_{KL}(P^{*}(y|x_{i})||P(y|x_{i};\theta)=\sum P^{*}(y|x_{i})log\frac{P^{*}(y|x_{i})}{P(y|x_{i};\theta)}}
\end{equation}

\end_inset


\size large
Rewriting the logarithm as two individual sections, we can observe that
 the left part of the equation below does not depend on the 
\begin_inset Formula $\theta$
\end_inset

 parameter.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{\sum\underbrace{P^{*}(y|x_{i})log(P^{*}(y|x_{i}))}_{\text{Independent of }\theta}-P^{*}(y|x_{i})P(y|x_{i};\theta)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\size large
And then we aim to make both distributions as similar as possible using
 the best estimator.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{\underset{\theta}{\text{argmin}}D_{KL}(P^{*}||P)\equiv\underset{\theta}{\text{argmin}}-\sum_{y}P^{*}(y|x_{i})P(y|x_{i};\theta)}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Effective Techniques for Improving Model Generalization
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
In the field of machine learning, it is important to ensure that the models
 we build are able to accurately and effectively make predictions on new
 data.
 However, it is common for models to suffer from issues such as overfitting
 or poor generalization to new data.
 In this section, we will explore three techniques that can be used to improve
 the performance of machine learning models: 
\series bold
regularization
\series default
, 
\series bold
normalization
\series default
, and 
\series bold
standardization
\series default
.
 By properly applying these techniques, we can mitigate the risks of overfitting
 and improve the ability of our models to generalize to new data.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Regularization_Bisong,Normalization_layers,standarization"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\shape italic
\size large
\emph on
\bar under
\color black
Our data set, which includes doubly dispersive channels of a complex nature,
 requires extra attention.
 We will use distorted vectors or inverse matrices, which can result in
 numerical instability, as our ground truth.
 These extra precautions will help to guarantee the process' success.
 Ignoring these recommendations may result in unsatisfactory outcomes, as
 the neural network may not generalize as well and may perform poorly.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ComplexnumbersNN"
literal "false"

\end_inset


\end_layout

\begin_layout Subsubsection
Regularization
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
Regularization is a technique used in machine learning to prevent overfitting.
 Overfitting occurs when a model is overly complex and captures the noise
 in the training data, rather than the underlying relationships.
 This results in poor generalization to new data.
 Regularization works by adding a penalty term to the objective function
 that the model is trying to minimize.
 This penalty term discourages the model from learning relationships that
 are too complex, and encourages it to learn simpler relationships that
 generalize better.
 There are several methods for regularization, including 
\begin_inset CommandInset citation
LatexCommand cite
key "Goodfellow-et-al-2016"
literal "false"

\end_inset

:
\end_layout

\begin_layout Enumerate
\align block

\size large
L1 regularization: This method adds a penalty term to the cost function
 that is proportional to the absolute value of the weights.
\end_layout

\begin_layout Enumerate
\align block

\size large
L2 regularization: This method adds a penalty term to the cost function
 that is proportional to the square of the weights.
\end_layout

\begin_layout Enumerate
\align block

\size large
Dropout regularization: This method randomly sets a fraction of the weights
 in the model to zero during training, which helps to prevent overfitting
 by reducing the number of parameters in the model.
 Dropout is only applied during the training process, and all neurons are
 available during evaluation.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename trasasdenivelRegularization.png
	lyxscale 50
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Level sets of the loss function and L1,L2 regularization 
\begin_inset CommandInset citation
LatexCommand cite
key "regimage"
literal "false"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
To implement regularization, you can modify the cost function of the model
 to include the regularization term.
 For example, in L2 regularization, the cost function would be modified
 to include the sum of the squares of the weights, as shown in the following
 equation:
\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{J(W)=\frac{\lambda}{2}||w||^{2}=\frac{\lambda}{2}\sum_{j=1}^{m}w_{j}^{2}}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize

\size larger
\begin_inset Formula $\boldsymbol{\lambda}$
\end_inset

 
\size large
regularization parameter
\end_layout

\begin_layout Itemize

\size larger
\begin_inset Formula $\boldsymbol{J(W)}$
\end_inset

 
\size large
Cost function
\end_layout

\begin_layout Subsubsection
Normalization 
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Data is scaled using a process called normalization to give it a unit norm
 (or length).
 This is frequently done to improve the data's suitability for particular
 machine learning algorithms, such as those that use gradient descent or
 have a set range for acceptable input data.
 When comparing various features, normalization can also be used to scale
 down the data to a common scale.
 Data normalization methods include min-max normalization, mean normalization,
 and z-score normalization.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\series bold
\size large
Normalization of complex numbers
\series default
 involves dividing a complex number by its magnitude (or absolute value)
 to obtain a complex number with a magnitude of 1.
 This is typically done to simplify calculations and make it easier to compare
 complex numbers.
 The normalized form of a complex number is often written as ẑ = z/|v|,
 where z is the original complex number and ẑ is the normalized form and
 |v| is the max magnitud of the entire vector of the complex numbers.
 Normalization of complex numbers is useful in many applications, including
 signal processing and control systems, where it is often necessary to compare
 complex numbers on an equal footing.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ComplexnumbersNN"
literal "false"

\end_inset


\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{ẑ=\frac{z}{|v|}}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\boldsymbol{|v|}$
\end_inset

 Maximum magnitud of the vector
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\mathsf{\boldsymbol{z}}$
\end_inset

 Input value
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\boldsymbol{ẑ}$
\end_inset

 Normalized value
\end_layout

\begin_layout Subsubsection
Standarization 
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
Standardization is a method used in machine learning to transform the values
 of a feature or set of features to a standard scale.
 The standard scale is typically defined as having a mean of 0 and a standard
 deviation of 1.
 Standardization is often used as a preprocessing step before training a
 model, as it can help to improve the performance and convergence of the
 model.
 Standardization can be useful when the features in the dataset have different
 scales or units, as it can help to bring them onto a common scale and make
 it easier for the model to learn from the data.
 Standardization can be applied to both real and complex-valued data.
\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{\hat{z}=\frac{z-\mu}{\sigma}}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\boldsymbol{\mu}$
\end_inset

 Mean of the data
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\boldsymbol{\sigma}$
\end_inset

 Standard deviation of the data
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\mathsf{\boldsymbol{z}}$
\end_inset

 Input value
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\boldsymbol{ẑ}$
\end_inset

 Standarized value
\end_layout

\begin_layout Subsubsection
Gradient clipping
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Gradient clipping is a technique used to prevent the gradients of a neural
 network from becoming too large during training.
 It is commonly used to mitigate the problem of exploding gradients, which
 can occur when the gradients become too large and cause the model's parameters
 to diverge.
 This means that if the gradient exceeds this threshold, it will be set
 to the threshold value.
 This effectively limits the size of the gradients and keeps them from becoming
 too large.
 
\begin_inset Formula $ $
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithm} 
\backslash
caption{Gradient Clipping} 
\end_layout

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout


\backslash
State threshold $
\backslash
gets$ max
\backslash
_norm 
\end_layout

\begin_layout Plain Layout


\backslash
For{each parameter p in the model}     
\end_layout

\begin_layout Plain Layout


\backslash
State $ 
\backslash
boldsymbol{g} 
\backslash
gets 
\backslash
frac{
\backslash
partial
\backslash
epsilon}{
\backslash
partial
\backslash
theta}$     
\end_layout

\begin_layout Plain Layout


\backslash
If{$
\backslash
left
\backslash
|
\backslash
boldsymbol{g}
\backslash
right
\backslash
| > threshold$}         
\end_layout

\begin_layout Plain Layout


\backslash
State $
\backslash
boldsymbol{g} 
\backslash
gets 
\backslash
frac{
\backslash
boldsymbol{g}}{
\backslash
left
\backslash
|grad
\backslash
right
\backslash
|} 
\backslash
times$ threshold     
\end_layout

\begin_layout Plain Layout


\backslash
EndIf     
\end_layout

\begin_layout Plain Layout


\backslash
State 
\end_layout

\begin_layout Plain Layout

update
\backslash
_parameter(p, grad) 
\end_layout

\begin_layout Plain Layout


\backslash
EndFor 
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic} 
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithm} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/GradiantClipping.png
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Gradient clipping
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Z-score
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The Z-score, also known as standard score, is a statistical measure that
 indicates how many standard deviations an observation or data point is
 from the mean of a data set.
 It is used to standardize data and compare individual observations to a
 population or sample mean.
 The magnitude of the z-score represents how far away the data point is
 from the mean in terms of standard deviations.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
This method has its own section in our results due to its relevance.
 It helps us to remove outliers and make our results more stable.
 Outliers are data points that are significantly different from the other
 data points in the data set and can skew statistical analyses or machine
 learning models.
 Z-score can be used to identify and reduce outliers in a data set.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
To identify outliers using z-score, we first calculate the z-score for each
 data point in the data set.
 We can then set a threshold z-score value, usually between 2 and 3, beyond
 which any data point is considered an outlier.
 Data points that have a z-score above the threshold are identified as outliers
 and can be removed from the data set.
 This can help to improve the accuracy and reliability of our results or
 predictions.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
\begin_inset Formula 
\begin{equation}
\boldsymbol{z=\frac{x_{i}-\mu}{\sigma}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
We use the z-score to filter outliers with a desired level of confidence,
 which is typically set at 95%.
 This corresponds to a z-score of 1.96, which means that approximately 95%
 of the data points would fall within the confidence interval.
 The 95% confidence level is commonly used in statistical analyses as a
 standard level of confidence
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/ConfidenceInterval.png
	lyxscale 70
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
95% Confidence interval 
\begin_inset CommandInset citation
LatexCommand cite
key "z_score_drawing"
literal "false"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Convolutional Neuronal Networks
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
A convolutional neural network (CNN) is a type of deep learning neural network
 that is primarily used for image and video recognition.
 It is designed to process data that has a grid-like topology, such as an
 image.
 The network is composed of multiple layers, including 
\series bold
convolutional layers
\series default
, 
\series bold
activation layers
\series default
, 
\series bold
pooling layers 
\series default
and
\series bold
 linear layers
\series default
.
 
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The convolutional layers apply a set of filters to the input data, where
 each filter is a small matrix of weights.
 This ones are used to identify features in the data such as 
\series bold
edges
\series default
, 
\series bold
textures
\series default
, and 
\series bold
shapes
\series default
, specifically, we will be utilizing these layers to extract the relationship
 of intercarrier symbol interference (ISI) and to perform dimensionality
 reduction.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Conv1"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Goodfellow-et-al-2016"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf

\size large
The 
\series bold
activation layers
\series default
 or activation functions introduce non-linearity to the network, allowing
 it to learn complex representations of the input data.
 
\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf

\size large
The 
\series bold
pooling layers
\series default
 reduce the spatial dimensions of the data, which helps to reduce overfitting
 and computational cost.
\end_layout

\begin_deeper
\begin_layout Itemize
\paragraph_spacing onehalf

\size large
Max pooling is used to pick the feature with the highest activation in a
 small region of the input feature map
\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf

\size large
Average pooling is used to reduce the spatial size of the input data by
 taking the average of the values of a small region of the input feature
 map.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Diagrams/Max_pool_avg_pool.jpg
	lyxscale 40
	scale 20

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Average and max pooling
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\noindent
\align block

\series bold
\size large
Linear layers
\series default
 classify the features extracted by the convolutional layers into the desired
 output.
\end_layout

\begin_layout Standard
\noindent
\align block

\size large
It should be noted that what is commonly referred to as 'convolution' in
 the context of convolutional neural networks (CNNs) is actually a cross-correla
tion operation, denoted with symbol 
\begin_inset Formula $\mathbf{\bigstar}$
\end_inset

 and convolution usually is used 
\begin_inset Formula $\boldsymbol{*}$
\end_inset

.
 The term 'convolution' is used only for convention purposes.
 The basic concept behind cross-correlation is to take a small matrix, referred
 to as a kernel or filter, and slide it over the input data (such as an
 image or audio signal).
 At each position, the kernel is multiplied element-wise with the underlying
 data, and the results are summed to produce a single output value, referred
 to as a feature map.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Diagrams/Conv2d_Kernel.jpg
	lyxscale 30
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Basic cross-correlation operation
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The output of the convolution operation is a feature map, where each element
 in the feature map is computed as follows:
\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{Y_{ij}=\sum K_{ab}\circ I_{i+a,j+b}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where kernel(a,b) is the value of the filter at position (a,b), input(i+a,j+b)
 is the value of the input at position (i+a,j+b) and output(i,j) is the
 output value at position (i,j)
\end_layout

\begin_layout Subsubsection
Channels
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
A channel refers to a specific feature or dimension of the input data.
 For example, in the case of image data, a channel can represent a color
 channel such as red, green, or blue.
 These channels are used to extract different features of the input image,
 and they are processed separately by the CNN.
 In our case study, we can use channels as a division between the real and
 imaginary parts, or for feature extraction of intercarrier symbol interference
 (ISI).
 We can have N channels as input and M channels as output, depending on
 how many features we want to deal with.
 In the image below, we show a case of 3 channel input and two channel output,
 also with a bias term.
\size default

\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/ConvolutionExpansion.png
	lyxscale 50
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Convolutional Neural Network with 3-channel Input and 2-channel Output,
 including bias term
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
A Generalized View of Linear Layers through Convolutional Neural Networks
\end_layout

\begin_layout Standard

\size large
Let's take a more detailed look at the math, given the following terms.
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\boldsymbol{j}$
\end_inset

 input channels with
\begin_inset Formula $\boldsymbol{X_{j}}$
\end_inset

 matrices
\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\boldsymbol{d}$
\end_inset

 ouput channels 
\begin_inset Formula $\boldsymbol{Y_{d}}$
\end_inset

matrices and this ones an output size of 
\begin_inset Formula $\boldsymbol{X_{j}-K_{ij}}$
\end_inset


\end_layout

\begin_layout Itemize

\size large
\begin_inset Formula $\boldsymbol{K_{ij}}$
\end_inset

 kernels,where 
\begin_inset Formula $\boldsymbol{i}$
\end_inset

 maps to 
\begin_inset Formula $\boldsymbol{Y_{d}}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{j}$
\end_inset

 to 
\begin_inset Formula $\boldsymbol{X_{j}}$
\end_inset


\end_layout

\begin_layout Itemize
\noindent
\align block

\size large
\begin_inset Formula $\mathbf{\bigstar}$
\end_inset

 cross-correlation
\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{Y_{d}=B_{d}+\sum_{j=1}^{n}X_{j}\bigstar K_{ij}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
We can think of the matrices as individual blocks and visualize them in
 the image below.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Conv2dGeneralization.png
	lyxscale 40
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Formula $Y_{d}$
\end_inset

 output given kernels
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Finally, with the use of abstraction, we can further simplify the problem
 by representing it as a generalized version of tensors.
 The internal tensor is given by the sum of the cross-correlations between
 X channels and kernels.
 In a more general perspective, this can be viewed as the inner product
 of two tensors.
\end_layout

\begin_layout Standard

\size larger
\begin_inset Formula 
\begin{equation}
\boldsymbol{Y=B+\langle K,X\rangle}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/MetaDense.png
	lyxscale 50
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Higher dimensionality abstract version 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
It's worth noting that when we consider a kernel of 1 dimension and an input
 of 1 channel, we can see that a dense layer is just a specific case of
 a 2D convolutional layer (Conv2D).
 Just as equation (15) and figure (8)
\end_layout

\begin_layout Subsection
Motivation
\end_layout

\begin_layout Subsection
Objectives
\end_layout

\begin_layout Subsubsection
General Objective
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
We should analyze and test different techniques of neural networks applied
 to frequency channel equalization, which is commonly used in OFDM.
 The NN models will be based on existing equalizers with well-known equations.
\end_layout

\begin_layout Subsubsection
Particular Objective
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align block

\size large
Our objective is to equalize a double dispersive channel, also known as
 a frequency-time channel and intercarrier symbol interference (ISI) through
 the utilization of various neural network strategies.
 These strategies will focus on specific stages of the existing equalization
 process.
 We aim to leverage the non-linearity of neural networks and employ techniques
 such as dimensionality expansion and reduction.
 Our aim is to recover QAM and PSK constellation symbols that have been
 distorted by a channel with both line-of-sight (LOS) and non-line-of-sight
 (NLOS) conditions, plus Gaussian noise.
 Our ultimate goal is to achieve a bit error rate (BER) comparable to that
 of the "golden models" or to reduce the computational complexity of current
 methods.
\end_layout

\begin_layout Section
State of Art
\end_layout

\begin_layout Subsection
Introduction
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\size large
To support our research and development, we conducted a thorough study of
 several key papers, which we organized into different categories based
 on their relevance to the project.
 In the following sections, I will provide a brief overview of how each
 of these articles contributed to the development of our project.
 Please note that many papers in the literature deal with smaller input
 and channel sizes than the 48x48 size used in our research.
 It is possible that these studies were mostly focused on proofs of concepts
 or making results fit better.
 Nevertheless, we opted for a larger channel size to create a more realistic
 scenario for our research.
 So BER/SNR metrics should be based on the golden models rather than on
 these articles.
\end_layout

\begin_layout Subsection
\paragraph_spacing onehalf
\noindent

\size large
The Roadmap to 6G – AI Empowered Wireless Networks 
\begin_inset CommandInset citation
LatexCommand cite
key "The_Roadmap_to_6G"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The integration of AI technologies in the communication network provides
 a promising future for efficient and reliable communication in 6G and beyond.
 The "intelligent PHY layer" paradigm, through its ability to self-learn
 and self-optimize, ensures that the system remains efficient and reliable
 despite the various hardware and channel effects.
 This model leverages AI technologies to enhance communication efficiency
 and performance, and can autonomously learn and enhance performance through
 the integration of cutting-edge sensing and data-gathering tools.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Hardware heterogeneity necessitates system redesign for different hardware
 settings, which can be overcome using transfer learning.
 This approach adjusts the neural network weights to work with custom hardware
 architecture, regardless of the training on floating point or fixed point
 backpropagation.
 Therefore, network architecture is more crucial than numeric resolution
 in contrast to traditional methods.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Papers/RoadMap6G.png
	lyxscale 50
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Roadmap showing the evolution of deep learning models in telecom and justifying
 our research
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
In conclusion, we present a roadmap in the image above depicting the expected
 evolution of deep learning in the coming years.
 This image justifies the necessity of our research.
 The picture shows the progression of models from simpler to more intelligent
 systems, with the intelligence level displayed in circles on the right
 side of the image.
\end_layout

\begin_layout Subsection
\paragraph_spacing onehalf
\noindent

\size large
A Novel OFDM Equalizer for Large Doppler Shift Channel through Deep Learning
 
\begin_inset CommandInset citation
LatexCommand cite
key "ZeroForcingEqNN"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
This documentation describes the proposed neural architecture called Cascaded
 Net (CN) for equalization in OFDM systems.
 The use of a zero-forcing preprocessor aims to prevent the network from
 getting stuck in a saddle point or local minimum point.
 The CN neural architecture is designed to address the equalization problem
 in OFDM systems with Rayleigh fading and large Doppler shifts.
 By cascading a deep trainable network behind a zero-forcing preprocessor,
 the CN architecture achieves superior performance in comparison to traditional
 equalization methods.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Papers/ZeroForcePreprocess.png
	lyxscale 50
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Deep learning equalizer with prepocesing stage
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
As they are performing frequency domain equalization, their base equation
 is the same as ours.
 
\begin_inset Formula $\boldsymbol{Y=Hx+W}$
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The Cascade net is defined as follows.
 
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
\begin_inset Formula 
\begin{equation}
\hat{\boldsymbol{X_{0}=(H^{H}H)^{-1}H^{H}Y}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
\begin_inset Formula 
\begin{equation}
\boldsymbol{z_{i}=w_{i}\left[\begin{array}{c}
H^{H}Y\\
\hat{X_{i}}\\
H^{H}H\hat{x_{i}}
\end{array}\right]+b_{i}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
\begin_inset Formula 
\begin{equation}
\boldsymbol{\hat{X_{i+1}}=\phi(z_{i})}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Where 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{b}$
\end_inset

 representes weights and bias as trainnable parameters.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
\begin_inset Formula 
\begin{equation}
\boldsymbol{\phi=tanh\left(\frac{x}{|t_{i}|}\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Papers/CascaNet.png
	lyxscale 50
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Cascade Net
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Their loss function or estimator is based on the Euclidean distance, with
 a logarithmic regularization of outliers and the objective of minimizing
 the distance between points.
 They accumulate the total estimation for each layer i and finally sum it
 up.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
\begin_inset Formula 
\begin{equation}
\mathbf{loss(X_{\theta}^{CN}(H,Y))=\sum_{i=1}^{L}log(i)||X-\hat{X||^{2}}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
To deal with complex matrix number they make a reformulation in the matrix
 as follows.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\size large
\begin_inset Graphics
	filename Imagenes/Papers/ComplexMatrix.png
	lyxscale 50
	scale 70

\end_inset


\size default

\begin_inset Caption Standard

\begin_layout Plain Layout
Matrix reformulation
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
BER perfomance
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The image below shows a benchmark of different equalizers, including ZF
 (Zero Forcing), PIC (Parallel Interference Cancellation), DET (Deep MIMO
 Detection Network)
\begin_inset CommandInset citation
LatexCommand cite
key "DeepMimoOFDM"
literal "false"

\end_inset

], and CN (Cascaded Net).
 The modulation they used in the experiment below is QPSK.
 and they assume that CSI is perfect.
 The receiver is trained off-line for the single-user system with fixed
 Doppler shift.
 If Subcarrier number 
\begin_inset Formula $N$
\end_inset

 is 32, 
\begin_inset Formula $f_{N}$
\end_inset

 equals to 0.16.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Graphics
	filename Imagenes/Papers/CascadeResults.png

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
\paragraph_spacing onehalf
\noindent

\size large
Classical PIC methods perform well in low subcarrier frequency scenarios
 (
\begin_inset Formula $f_{N}$
\end_inset

 less than 0.1).
 However, their performance degrades significantly with an increase in subcarrie
r frequency due to significant error propagation, as stated in the first
 section.
\end_layout

\begin_layout Enumerate
\paragraph_spacing onehalf
\noindent

\size large
Deep MIMO detection (Det) faces a flat error curve in high SNR scenarios.
\end_layout

\begin_layout Enumerate
\paragraph_spacing onehalf
\noindent

\size large
Cascaded Net (CN) consistently performs well compared to Zero Forcing (ZF)
 and Det, which is in line with the argument made in the third section.
\end_layout

\begin_layout Subsubsection
Performance Improvement Strategies for our solutions
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\size large
As a conclusion, we have opted to use 
\begin_inset Formula $\boldsymbol{H^{H}Y}$
\end_inset

 instead of pure 
\begin_inset Formula $\boldsymbol{Y}$
\end_inset

 as an input for certain neural networks in our research.
 We have also implemented zero forcing as an additional preprocessing stage.
 These strategies have proven to be effective as they bring constellation
 points closer to one another, reducing the network's burden to equalize
 the signal to the ideal state, particularly when dealing with long distances
 between ideal point and recieved point.
 
\end_layout

\begin_layout Subsection
\paragraph_spacing onehalf
\noindent

\size large
Recent Advances in Neural Network Techniques For Channel Equalization:A
 Comprehensive Survey 
\begin_inset CommandInset citation
LatexCommand cite
key "Recent_Advances_in_Neural_Network_Techniques_for_Channel_Equalization"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\size large
This paper provides an overview of various channel equalization methods,
 including the Multilayer Perceptron (MLP) equalizer, Functional Link Artificial
 Neural Network (FLANN) equalizer, Chebyshev Neural Network (NN) equalizer,
 and Radial Basis Function NN (RBFNN) equalizer.
 Additionally, it presents a literature review and application of Recursive
 Neural Network (RNN) and Fuzzy Neural Network equalizers.
\end_layout

\begin_layout Subsubsection
FLANN
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\size large
The primary difference between FLANN hardware and MLP configuration is that
 the nonlinear mapping replaces only the input, output, and hidden layers.
 This mapping uses a non-linear function to transform the input vector,
 mapping it to a higher-dimensional space.
 The expansion function, called the functional link, is typically a polynomial
 function of the input variables.
 In our final experiments, we explored the concept of searching for equalization
 in a higher-dimensional space, but did not utilize the polynomial function
 approach.
\end_layout

\begin_layout Subsubsection
RBF
\end_layout

\begin_layout Standard

\size large
Radial basis functions (RBFs) are a type of basis function used in function
 approximation and machine learning algorithms.
 RBFs are a class of functions that depend only on the distance from the
 center of the function, and their output decreases as the distance from
 the center increases.
 Gaussian RBF: This function takes the form of 
\begin_inset Formula $e^{(-r^{2}/2)}$
\end_inset

, where r is the Euclidean distance from the center of the function.
 This RBF is widely used in machine learning and function approximation
 algorithms due to its smoothness and symmetry.
\end_layout

\begin_layout Subsubsection
RNN
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\size large
The article concludes by stating that Recurrent Neural Networks (RNNs) generally
 outperform feed-forward neural networks (FNNs) and other methods.
 RNNs approximate a finite impulse response (IIR) filter, whereas other
 methods approximate a finite impulse response (FIR) filter.
 It is worth noting that IIR filters are known to be unstable, but recent
 advances in neural networks, such as LSTM, GRU, and Transformers, have
 been developed to overcome this limitation 
\end_layout

\begin_layout Subsubsection
Conclusion
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\size large
The article discusses various approaches using different network architectures.
 However, it lacks any BER/SNR plots and focuses solely on QPSK study.
 According to this research, Recurrent Neural Networks (RNNs) appear to
 be the most effective solution, rendering other proposed strategies unnecessary.
 Given the significant advancements in RNNs in recent years and the emergence
 of powerful tools like transformers, we should prioritize working with
 transformers, also there is nothing similar to transformers in the existing
 literature.
 
\end_layout

\begin_layout Subsection
\paragraph_spacing onehalf
\noindent

\size large
An Introduction to Deep Learning for the Physical Layer 
\begin_inset CommandInset citation
LatexCommand cite
key "Intro_DL_Physical_Layer"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\noindent

\size large
This paper provides a detailed overview of neural networks and their application
 to channel equations, with mathematical proofs included.
 The paper describes the formal structure of feed-forward neural networks,
 as well as some applications of convolutional neural networks.
 It also introduces autoencoders for end-to-end communication systems and
 proposes the idea that an autoencoder can be used to characterize a complete
 channel.
\end_layout

\begin_layout Subsection
\paragraph_spacing onehalf
\noindent

\size large
A Survey of Complex-Valued Neural Networks 
\begin_inset CommandInset citation
LatexCommand cite
key "A_Survey_of_Complex_valued_nueronal_Netoworks"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\noindent

\size large
As we know, IQ data is represented by complex numbers, which can make it
 challenging to work with.
 This paper provides a detailed description of a new loss function and adjustmen
ts needed to describe backpropagation in complex networks.
 The paper also shows how to evaluate cost functions, which is helpful for
 conducting experiments with complex neural networks.
\end_layout

\begin_layout Subsubsection
\paragraph_spacing onehalf
\noindent

\size large
Searching for MobileNetV3 
\begin_inset CommandInset citation
LatexCommand cite
key "MobilenetV3"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\noindent

\size large
MobileNet achieves its high efficiency by using depthwise separable convolutions
, which are a combination of a depthwise convolution and a pointwise convolution.
ing depthwise separable convolutions, this significantly reduces the number
 of parameters and computations required for a convolution operation, while
 still maintaining high accuracy.
 We used MobileNet in one of our experiments and compared it with the zero
 forcing equalizer.
 Since both methods are expected to be faster but less accurate, we decided
 to compare them as a part of our study
\end_layout

\begin_layout Subsection
\paragraph_spacing onehalf
\noindent

\size large
Attention Is All You Need 
\begin_inset CommandInset citation
LatexCommand cite
key "vaswani2017attention"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\noindent

\size large
According to "Attention Is All You Need", a research paper on neural network
 architecture, the Transformer is a new architecture that utilizes attention
 mechanisms, instead of traditional recurrent or convolutional layers, to
 process sequential input data, such as natural language text.
 This architecture has demonstrated state-of-the-art performance in a variety
 of natural language processing tasks, including language modeling and machine
 translation.
\end_layout

\begin_deeper
\begin_layout Itemize
\paragraph_spacing onehalf
\noindent

\size large
Given that our data is sequential in nature and received symbol by symbol,
 we could apply the Transformer architecture in our equalization stage.
 The Transformer uses an attention mechanism that enables it to process
 all input data in parallel, thereby computing the importance of each input
 element relative to all other elements in the sequence.
 This information is then used to weight the contribution of each element
 to the output, and we believe that this resulting output could be an effective
 method for canceling Inter-Symbol Interference (ISI) using attention mechanisms
\end_layout

\end_deeper
\begin_layout Subsection
\paragraph_spacing onehalf
\noindent

\size large
An image is worth 16x16 words 
\begin_inset CommandInset citation
LatexCommand cite
key "dosovitskiy2020image"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf
\noindent

\size large
An Image is Worth 16x16 Words" is a research paper that introduces a new
 type of neural network architecture called the Vision Transformer (ViT),
 which applies the Transformer architecture to image recognition tasks.
 The ViT splits the input image into patches, which are then flattened and
 transformed using self-attention mechanisms.
 This approach allows the ViT to capture both local and global features
 of the image and has achieved state-of-the-art performance in several image
 recognition benchmarks.
\end_layout

\begin_deeper
\begin_layout Itemize
\paragraph_spacing onehalf
\noindent

\size large
We could treat the IQ map as an 
\begin_inset Quotes eld
\end_inset

image
\begin_inset Quotes erd
\end_inset

 of points and give a discrete numeric number based on a grid.
 We describe more in detail in the results section.
 
\end_layout

\end_deeper
\begin_layout Section
Results and Analysis
\end_layout

\begin_layout Subsection
Dataset
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
This project introduces a dataset of 20,000 channel realizations, each with
 a size of 48x48.
 The dataset is divided into two groups of 10,000, with the first group
 consisting of Line-of-Sight (LOS) channel realizations and the second group
 consisting of Non-Line-of-Sight (NLOS) channel realizations.
 Each value in the matrix is a complex number with a format of complex128,
 float64 for real and imaginary parts.
 The data is stored in the .mat format, which is a common format used for
 storing variables on disk from Matlab code.
 However, the data can also be used in Python using the Scipy library.
 To ensure a robust prediction, the LOS and NLOS channels have been shuffled,
 with one following the other.
 As neural networks do not typically work well with complex numbers, the
 real and imaginary parts of the channels have been separated and used as
 two separate image channels.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Channels/NLOS_Channel_log_view.png
	lyxscale 70
	scale 50

\end_inset


\begin_inset Graphics
	filename Imagenes/Channels/LOS_Channel_log_view.png
	lyxscale 70
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
A comparison of LOS and NLOS channel magnitud in a log scale representation.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Golden model
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The golden model, as defined in this project, serves as a benchmark for
 evaluating the performance of new models.
 The golden model is based on the Least squares (LS) and Linear Minimum
 Mean Squared (LMMSE) Error equalizers, which are discussed in further detail
 in the equalization section of this document.When evaluating the performance
 of a communication system, it is standard practice to plot both the Bit
 Error Rate (BER) and Signal-to-Noise Ratio (SNR) on the same graph, with
 SNR on the x-axis and BER on the y-axis.
 BER is a measure of the number of errors that occur in a transmitted data
 stream, relative to the total number of bits transmitted.
 SNR, on the other hand, is a measure of the strength of the signal relative
 to the background noise.
 By comparing the BER and SNR values, it is possible to evaluate the performance
 of the system under different levels of noise.
 A decrease in BER as SNR increases indicates that the system is able to
 transmit data more accurately.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Results/Golden_4QAM.png
	lyxscale 40
	scale 55

\end_inset


\begin_inset Graphics
	filename Imagenes/Results/Golden_16QAM.png
	lyxscale 40
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Golden model plot QPSK and 16QAM 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Software 
\size large
Architecture
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
In the field of machine learning, data is typically split into three sets:
 training, validation, and testing.
\end_layout

\begin_layout Enumerate
\paragraph_spacing onehalf

\series bold
\size large
Training set
\series default
: The training set is a subset of the data used to train the model.
 The model uses the training data to learn the relationships between the
 input features and the target output.
 The model parameters are updated during the training process to minimize
 the prediction error.
\end_layout

\begin_layout Enumerate
\paragraph_spacing onehalf
\noindent

\series bold
\size large
Validation set
\series default
: The validation set is a subset of the data used to evaluate the model
 during training.
 The purpose of the validation set is to ensure that the model is not overfittin
g to the training data.
 Overfitting occurs when the model is too complex and learns the noise in
 the training data instead of the underlying relationships.
 The model is evaluated on the validation set after each training epoch,
 and its performance is used to determine when to stop training or to adjust
 the model's hyperparameters.
\end_layout

\begin_layout Enumerate
\paragraph_spacing onehalf
\noindent

\series bold
\size large
Testing set
\series default
: The testing set is a subset of the data used to evaluate the model's performan
ce after training.
 The model is never trained on the test set and its performance on the test
 set provides an estimate of its generalization performance to new, unseen
 data.
 The test set is used to determine the final accuracy of the model and its
 ability to make predictions on new data.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
Also, a modular and maintainable code with class inheritance and shared
 attributes is necessary for code reusability.
 Given that the project will undergo multiple experiments based on the same
 code, a generic software architecture that can consume different instances
 has been developed.
 This, combined with the PyTorch Lightning framework, has made the development
 process much easier and faster.
 PyTorch Lightning also offers tools for distributed training that can be
 used to scale the training of big models across several GPUs, TPUs, or
 machines.
 This tooling also facilitates the concept of batches, which involves having
 multiple realizations of the data set in the format [BATCH, sequlen] or
 [BATCH, channel, height, width].
 By using batches, the training time is reduced and it becomes more manageable
 to handle large amounts of data.
 
\end_layout

\begin_layout Standard

\size large
These are the main classes used in the software development
\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf

\series bold
\size large
Received class
\series default
 utilizes the channel data set to generate 20,000 x 48 random points in
 a QAM constellation.
 This class provides the data loader elements to be loaded, which will produce
 a tuple of values including the channels and the tx QAM.
 
\end_layout

\begin_layout Itemize

\series bold
\size large
Data loader class
\series default
 divides the data into the previously mentioned three sets, making it easily
 accessible to the data processing stage.
 Within the data processing stage, there are various functions.
\end_layout

\begin_deeper
\begin_layout Itemize

\shape italic
\size large
Get Y
\shape default
: Takes the channels and multiplies them by the tx QAM.
 This function can also receive the noise activation as a parameter.
\end_layout

\begin_layout Itemize
\paragraph_spacing onehalf

\shape italic
\size large
LS/LMSSE
\shape default
: This function was made because in some training scenarios, the ground
 truth can originate from these functions.
 Also to feed the Golden model.
 
\end_layout

\begin_layout Itemize

\shape italic
\size large
SNR_BER_Test
\shape default
 and BER calc
\shape italic
:
\shape default
 This functions are utilized to calculate the final metrics of the model
 after training.
 The results are saved in a CSV file, containing the BER and SNR values.
 These results can be later plotted by reading the CSV file using the pandas
 library and plotting it using the matplotlib library.
\end_layout

\end_deeper
\begin_layout Standard
\paragraph_spacing onehalf

\size large
The software architecture will be displayed in the image below.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Codigo/SoftwareDiagrams/networkdevelop.png
	lyxscale 50
	scale 28

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Software Architecture Diagram
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Ultra-Lightweight Networks
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The Ultra-Lightweight Networks are designed with a core network consisting
 of three linear layers and three hardtanh functions.
 The input is multiplied by the Hermitian of the channel, which makes it
 easier for the network to converge as QAM points become closer together.
 Additionally, the IQ data is normalized to have a maximum magnitude of
 1 by dividing all elements by the maximum magnitude value of the data.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The training of these Ultra-Lightweight Networks is performed using Mean
 Squared Error (MSE) loss estimation.
 One advantage of these networks is that their processing can be easily
 implemented on embedded systems or FPGA designs, as the hardtanh functions
 used are simply linear clipping functions.
 Furthermore, these methods are fast in processing compared to traditional
 methods, making them a suitable choice for equalization.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
However, it should be noted that while these methods perform well in noisy
 scenarios, they may not be as effective in clean scenarios.
 This trade-off should be considered when deciding to use these methods.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Name 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Total Parameters
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Estimated memory size
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Phase Net
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
12.3 K
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
49 KB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Polar Net
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
42 K 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
168 KB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Real Imag Net
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
24.6 K 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
98 KB
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Network size Benchmark
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Results/Models_4QAM.png
	lyxscale 40
	scale 80

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Golden Model vs Networks in 4-QAM
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Results/Models_16QAM.png
	lyxscale 30
	scale 67

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Golden Model Vs Networ in 16-QAM PhaseNet not included
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Phase Net
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The PhaseNet is a specialized neural network that is designed to operate
 in systems where the amplitude information of signals is not critical,
 and bits are encoded based on the angles they occupy.
 Despite its limited performance compared to other methods, such as the
 Least Mean Squared Error (LMSSE), PhaseNet offers a faster computational
 approach, making it a more efficient alternative.
 The computation of the channel inverse, for example, is time-consuming,
 while PhaseNet offers a more computationally efficient solution.
 This makes PhaseNet a desirable option in situations where computational
 speed is a priority.
 However, it is important to note that its accuracy may decrease for SNRs
 greater than 20dB.
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The PhaseNet training produced an error of 
\begin_inset Formula $10^{-4}$
\end_inset

, which is considered one of the smaller errors among comparable neural
 networks.
 The training process consisted of 100,000 iterations with a batch size
 of 10.
 The constant validation loss observed during the training suggests that
 the model does not overfit.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Networks/PhaseNet.png
	lyxscale 60
	scale 45

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Phase Net Layers
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Networks/Phanet_train_loss.png
	lyxscale 30
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Imagenes/Networks/Phanet_val_loss.png
	lyxscale 30
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Phase Net Trainning and validation loss
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Polar Net
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The PolarNet consists of two networks: one for magnitude and the other for
 phase.
 The final output from both modules is concatenated to form a 48x2 matrix
 that serves as the estimator output.
 The ground truth has the same shape.
 The input data of the PolarNet is normalized with respect to the maximum
 absolute value in the batch.
 The angle is mapped from 
\begin_inset Formula $\boldsymbol{[-\pi,\pi]}$
\end_inset

 to [0, 1] using the equation
\size largest
 
\begin_inset Formula $\boldsymbol{\frac{\theta-\pi}{2\pi}}$
\end_inset


\size large
.
 This data mapping helps the network make better predictions and converge
 faster.
 It's important to note that the mean squared error (MSE) calculation does
 not take into account the complex representation of the data and only considers
 the values as two 2D spatial values, one for magnitude and the other for
 phase.
\end_layout

\begin_layout Subsubsection
Real-Imaginary Net
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
The Real Imaginary Net is similar to the Polar Net, with the difference
 being that it concatenates the real and imaginary parts.
 With low Signal-to-Noise Ratio (SNR), it performs similarly to the Polar
 Net, but rapidly, the Bit Error Rate (BER) flattens regardless of the increase
 in SNR.
 During the experimentation process, we attempted to use the distance between
 the target and output as an estimator for the network, but it performed
 worse than when using Mean Squared Error (MSE) estimation.
\end_layout

\begin_layout Section
Conclusion and Outlook
\end_layout

\begin_layout Subsection
Future Work
\end_layout

\begin_layout Section
References
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf

\size large
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "Bibliography"
options "plain"

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard

\end_layout

\end_body
\end_document
